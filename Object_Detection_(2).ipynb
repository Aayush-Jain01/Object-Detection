{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_Detection_(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bebde1dc279e4525b9f8602a0753959c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e40fc3a37434ee2b2785cd7402bc610",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_976a55a145f34854b97cc1a3cc1e8b87",
              "IPY_MODEL_94394e3e873748ee9efe15e53b87b88b",
              "IPY_MODEL_a3ac5143349a4ce89206cf4c5060fdd9"
            ]
          }
        },
        "4e40fc3a37434ee2b2785cd7402bc610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "976a55a145f34854b97cc1a3cc1e8b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9b3af823aab4ffe99e56a86c1ce7ace",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bf94f77f160472397b5259badfa3eb6"
          }
        },
        "94394e3e873748ee9efe15e53b87b88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8ddf8048383e4e99bf3fcb8c3920d151",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d33bb00d7db48c2b9a83cf109253f03"
          }
        },
        "a3ac5143349a4ce89206cf4c5060fdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41767b6b5939461a9352821fd90d12a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:01&lt;00:00, 105MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6785f2ef086f4cebadf73f08a4e766a6"
          }
        },
        "b9b3af823aab4ffe99e56a86c1ce7ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bf94f77f160472397b5259badfa3eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ddf8048383e4e99bf3fcb8c3920d151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d33bb00d7db48c2b9a83cf109253f03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41767b6b5939461a9352821fd90d12a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6785f2ef086f4cebadf73f08a4e766a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush-Jain01/Object-Detection/blob/main/Object_Detection_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss98KB0WofqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3cc903-a889-4215-9db5-64a6591f050a"
      },
      "source": [
        "!pip install ray[tune]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[tune]\n",
            "  Downloading ray-1.7.1-cp37-cp37m-manylinux2014_x86_64.whl (54.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.0 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.13)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.19.5)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.41.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.3.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 516 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.1.5)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[tune]) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
            "Installing collected packages: redis, tensorboardX, ray\n",
            "Successfully installed ray-1.7.1 redis-3.5.3 tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_aqZ8ndosPi"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "import numpy as np\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import io as F \n",
        "import datetime\n",
        "import errno\n",
        "from collections import defaultdict, deque\n",
        "import torch.distributed as dist\n",
        "from contextlib import redirect_stdout\n",
        "from collections import defaultdict, deque\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.rpn import AnchorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFJlNw2D3gf",
        "outputId": "3adcf0ac-1595-4ef3-f6ce-37f1f50415a1"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u-JVbcEotxY"
      },
      "source": [
        "class CarDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform):\n",
        "    super().__init__()\n",
        "    self.csv_file = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.csv_file)\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.root_dir, self.csv_file.iloc[idx, 0])\n",
        "    image = io.imread(img_path)                     #Reading the image\n",
        "    image_ = Image.fromarray(image)                 #Converting image to PIL to be able to apply the transforms\n",
        "    bounding_params = list(self.csv_file.iloc[idx, 1:])   \n",
        "    #bounding_params = np.array([bounding_params])\n",
        "    bounding_params = torch.as_tensor(bounding_params, dtype= torch.float32)\n",
        "    bounding_params = torch.reshape(bounding_params, (1, 4))\n",
        "    labels = torch.ones((1, ), dtype= torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = torch.as_tensor([(bounding_params[0][3]-bounding_params[0][1])*(bounding_params[0][2]-bounding_params[0][0])])\n",
        "    iscrowd = torch.zeros((1, ), dtype=torch.int64)\n",
        "    target = {}\n",
        "    target[\"boxes\"] = bounding_params\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    #sample = {'image': image, \"bounding_parms\" : bounding_params}\n",
        "    if self.transform:\n",
        "        image_1 = self.transform(image_)\n",
        "    return image_1, target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXdNyilwEAxz",
        "outputId": "ca22f91e-e661-4015-c1ee-554fa57e23ae"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "        #transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        #transforms.RandomRotation(degrees=15),\n",
        "        #transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        #transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             #[0.229, 0.224, 0.225])  # Imagenet standards\n",
        "      ])\n",
        "test_transforms = transforms.Compose([\n",
        "        #transforms.Resize(size=256),\n",
        "        #transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "train_set = CarDataset(\"/content/drive/MyDrive/YOLO Object Detection/train_solution_bounding_boxes (1).csv\", \"/content/drive/MyDrive/YOLO Object Detection/training_images/\", transform= train_transforms)\n",
        "train_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CarDataset at 0x7f15584f6b10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYkXgPbnLbhJ",
        "outputId": "5750de8c-7298-4275-ba32-d0fe60255bd9"
      },
      "source": [
        "train_set.__getitem__(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.3882, 0.3922, 0.3922,  ..., 0.4980, 0.4980, 0.5020],\n",
              "          [0.4000, 0.4000, 0.4000,  ..., 0.4980, 0.5020, 0.5020],\n",
              "          [0.4118, 0.4078, 0.4078,  ..., 0.5020, 0.5059, 0.5059],\n",
              "          ...,\n",
              "          [0.0824, 0.0824, 0.0824,  ..., 0.1059, 0.1098, 0.1020],\n",
              "          [0.0824, 0.0824, 0.0824,  ..., 0.1059, 0.1059, 0.0941],\n",
              "          [0.0784, 0.0784, 0.0784,  ..., 0.1098, 0.1098, 0.0980]],\n",
              " \n",
              "         [[0.7137, 0.7176, 0.7255,  ..., 0.8235, 0.8235, 0.8275],\n",
              "          [0.7137, 0.7137, 0.7137,  ..., 0.8235, 0.8275, 0.8275],\n",
              "          [0.7098, 0.7137, 0.7137,  ..., 0.8275, 0.8314, 0.8314],\n",
              "          ...,\n",
              "          [0.1686, 0.1686, 0.1686,  ..., 0.2000, 0.2000, 0.1922],\n",
              "          [0.1686, 0.1686, 0.1686,  ..., 0.2000, 0.1961, 0.1843],\n",
              "          [0.1647, 0.1647, 0.1647,  ..., 0.2039, 0.2000, 0.1882]],\n",
              " \n",
              "         [[0.9725, 0.9765, 0.9804,  ..., 0.9961, 0.9961, 1.0000],\n",
              "          [0.9922, 0.9922, 0.9922,  ..., 0.9961, 1.0000, 1.0000],\n",
              "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9961, 0.9961],\n",
              "          ...,\n",
              "          [0.2627, 0.2627, 0.2627,  ..., 0.3098, 0.3216, 0.3137],\n",
              "          [0.2627, 0.2627, 0.2627,  ..., 0.3098, 0.3176, 0.3059],\n",
              "          [0.2588, 0.2588, 0.2588,  ..., 0.3137, 0.3216, 0.3098]]]),\n",
              " {'area': tensor([1681.7317]),\n",
              "  'boxes': tensor([[281.2590, 187.0351, 327.7279, 223.2255]]),\n",
              "  'image_id': tensor([0]),\n",
              "  'iscrowd': tensor([0]),\n",
              "  'labels': tensor([1])})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z19L8o0JF5Zy"
      },
      "source": [
        "def collate_func(batch):\n",
        "    return tuple(zip(*batch))\n",
        "train_set, val_set = random_split(train_set, lengths=[447,112])\n",
        "test_set = CarDataset(\"/content/drive/MyDrive/YOLO Object Detection/sample_submission.csv\", \"/content/drive/MyDrive/YOLO Object Detection/testing_images\", test_transforms)\n",
        "train_dl =  DataLoader(train_set, batch_size= 3, collate_fn= collate_func, shuffle = True)\n",
        "val_dl = DataLoader(val_set, batch_size= 4, collate_fn= collate_func, shuffle = True)\n",
        "test_dl = DataLoader(test_set, batch_size = 4, collate_fn = collate_func, shuffle = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Sq5Rc7DQWg"
      },
      "source": [
        "from torchvision import models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXhl5W7f_8Xu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bebde1dc279e4525b9f8602a0753959c",
            "4e40fc3a37434ee2b2785cd7402bc610",
            "976a55a145f34854b97cc1a3cc1e8b87",
            "94394e3e873748ee9efe15e53b87b88b",
            "a3ac5143349a4ce89206cf4c5060fdd9",
            "b9b3af823aab4ffe99e56a86c1ce7ace",
            "0bf94f77f160472397b5259badfa3eb6",
            "8ddf8048383e4e99bf3fcb8c3920d151",
            "9d33bb00d7db48c2b9a83cf109253f03",
            "41767b6b5939461a9352821fd90d12a0",
            "6785f2ef086f4cebadf73f08a4e766a6"
          ]
        },
        "outputId": "4a4c882d-380a-4b49-9c79-dfca8f09a2ae"
      },
      "source": [
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "for parameters in model.parameters():\n",
        "  parameters.requires_grad = False\n",
        "num_classes = 2 \n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bebde1dc279e4525b9f8602a0753959c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoaF9-9LKyFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1e7d95-2223-43a8-ebfb-55ef694c86e2"
      },
      "source": [
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3VeMfMQQfep",
        "outputId": "1b4a9da3-69b0-4e64-c0de-e6abf7fca275"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'vision'...\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIFr-JvSQrx-"
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCQZ9lqgQSS4"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "# our dataset has two classes only - raccoon and not racoon\n",
        "num_classes = 2\n",
        "# get the model using our helper function\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbYGqizVI_DD"
      },
      "source": [
        "\n",
        "def eval_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    model.eval()\n",
        "    print(\"---------Evaluating-----------\")\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = \"Epoch: [{}]\".format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    # if epoch == 0:\n",
        "    #     warmup_factor = 1.0 / 1000\n",
        "    #     warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "    #     lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    #         optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "    #     )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFgIIFcT_1ae"
      },
      "source": [
        "# def train(config, checkpoint_dir=None, data_dir=None):\n",
        "#     optimizer = torch.optim.Adam(params, config[\"lr\"], betas= (config[\"b1\"], config[\"b2\"]))\n",
        "#     # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
        "#     #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "#     #lr_schedular = torch.optim.lr_scheduler.CyclicLR(optimizer, config[\"base_lr\"], config[\"max_lr\"], gamma=config[\"gamma\"]\n",
        "#     if checkpoint_dir:\n",
        "#         model_state, optimizer_state = torch.load(\n",
        "#             os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "#         model.load_state_dict(model_state)\n",
        "#         optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "#     train_dl =  DataLoader(train_set, batch_size= int(config[\"batch_size\"]), collate_fn= collate_func, shuffle = True, num_workers=8)\n",
        "#     val_dl = DataLoader(val_set, batch_size= int(config[\"batch_size\"]), collate_fn= collate_func, shuffle = True, num_workers=8)\n",
        "#     test_dl = DataLoader(test_set, batch_size= int(config[\"batch_size\"]), collate_fn = collate_func, shuffle = False)\n",
        "\n",
        "#     for epoch in range(10):  # loop over the dataset multiple times\n",
        "#         train_one_epoch(model, optimizer, train_dl, device, epoch,\n",
        "#                       print_freq=10)\n",
        "\n",
        "#         #lr_scheduler.step()\n",
        "\n",
        "#         a = eval_one_epoch(model, optimizer, val_dl, device, epoch,\n",
        "#                       print_freq=10)\n",
        "        \n",
        "#         with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "#             path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "#             torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "#         tune.report(loss= a)\n",
        "#     print(\"Finished Tuning !\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YFcrwQRQSgr"
      },
      "source": [
        "def train(config):\n",
        "    # optimizer = torch.optim.Adam(params, config[\"lr\"], betas= (config[\"b1\"], config[\"b2\"]))\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr= config[\"lr\"], weight_decay= config[\"weight_decay\"])\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr= config[\"base_lr\"], max_lr = config[\"max_lr\"], gamma= config[\"gamma\"]) \n",
        "    # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
        "    #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    # lr_schedular = torch.optim.lr_scheduler.CyclicLR(optimizer, config[\"base_lr\"], config[\"max_lr\"], gamma=config[\"gamma\"])\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "      # train for one epoch, printing every 10 iterations\n",
        "      train_one_epoch(model, optimizer, train_dl, device, epoch,\n",
        "                      print_freq=10)\n",
        "    # update the learning rate\n",
        "      lr_scheduler.step()\n",
        "      # evaluate on the test dataset\n",
        "      evaluate(model, val_dl, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aXweHNl9dtu_",
        "outputId": "916cab75-2245-4939-dac7-fea3407e60f9"
      },
      "source": [
        "config = {\n",
        "    # \"lr\" : 1e-5,\n",
        "    # \"b1\" : 0.9,\n",
        "    # \"b2\" : 0.99,\n",
        "    # \"batch_size\" : 4,\n",
        "    # \"base_lr\" : 1e-5,\n",
        "    # \"max_lr\" : 5e-3,\n",
        "    # \"gamma\" : 1.0\n",
        "    \"lr\" : 1e-4,\n",
        "    \"weight_decay\" : 1e-5,\n",
        "    \"base_lr\" : 1e-5,\n",
        "    \"max_lr\" : 5e-3,\n",
        "    \"gamma\" : 1.0}\n",
        "train(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/149]  eta: 0:08:38  lr: 0.000000  loss: 0.5598 (0.5598)  loss_classifier: 0.4588 (0.4588)  loss_box_reg: 0.0405 (0.0405)  loss_objectness: 0.0468 (0.0468)  loss_rpn_box_reg: 0.0137 (0.0137)  time: 3.4831  data: 1.8995  max mem: 1388\n",
            "Epoch: [0]  [ 10/149]  eta: 0:06:26  lr: 0.000001  loss: 0.5955 (0.5957)  loss_classifier: 0.4723 (0.4670)  loss_box_reg: 0.0614 (0.0690)  loss_objectness: 0.0439 (0.0501)  loss_rpn_box_reg: 0.0072 (0.0096)  time: 2.7800  data: 1.6206  max mem: 1388\n",
            "Epoch: [0]  [ 20/149]  eta: 0:05:46  lr: 0.000001  loss: 0.5956 (0.5931)  loss_classifier: 0.4689 (0.4687)  loss_box_reg: 0.0614 (0.0685)  loss_objectness: 0.0378 (0.0463)  loss_rpn_box_reg: 0.0070 (0.0097)  time: 2.6475  data: 1.5324  max mem: 1388\n",
            "Epoch: [0]  [ 30/149]  eta: 0:05:09  lr: 0.000002  loss: 0.5926 (0.5894)  loss_classifier: 0.4622 (0.4652)  loss_box_reg: 0.0648 (0.0702)  loss_objectness: 0.0370 (0.0449)  loss_rpn_box_reg: 0.0070 (0.0091)  time: 2.5068  data: 1.3916  max mem: 1388\n",
            "Epoch: [0]  [ 40/149]  eta: 0:04:38  lr: 0.000003  loss: 0.5844 (0.5863)  loss_classifier: 0.4544 (0.4613)  loss_box_reg: 0.0645 (0.0672)  loss_objectness: 0.0426 (0.0481)  loss_rpn_box_reg: 0.0075 (0.0097)  time: 2.4225  data: 1.3009  max mem: 1388\n",
            "Epoch: [0]  [ 50/149]  eta: 0:04:10  lr: 0.000003  loss: 0.5665 (0.5833)  loss_classifier: 0.4378 (0.4550)  loss_box_reg: 0.0587 (0.0676)  loss_objectness: 0.0662 (0.0511)  loss_rpn_box_reg: 0.0093 (0.0096)  time: 2.4160  data: 1.2961  max mem: 1388\n",
            "Epoch: [0]  [ 60/149]  eta: 0:03:42  lr: 0.000004  loss: 0.5617 (0.5769)  loss_classifier: 0.4260 (0.4493)  loss_box_reg: 0.0580 (0.0680)  loss_objectness: 0.0473 (0.0503)  loss_rpn_box_reg: 0.0078 (0.0093)  time: 2.3931  data: 1.2808  max mem: 1388\n",
            "Epoch: [0]  [ 70/149]  eta: 0:03:13  lr: 0.000005  loss: 0.5529 (0.5746)  loss_classifier: 0.4079 (0.4416)  loss_box_reg: 0.0580 (0.0698)  loss_objectness: 0.0473 (0.0538)  loss_rpn_box_reg: 0.0084 (0.0094)  time: 2.2263  data: 1.1135  max mem: 1388\n",
            "Epoch: [0]  [ 80/149]  eta: 0:02:47  lr: 0.000005  loss: 0.5295 (0.5679)  loss_classifier: 0.3755 (0.4326)  loss_box_reg: 0.0637 (0.0717)  loss_objectness: 0.0649 (0.0538)  loss_rpn_box_reg: 0.0087 (0.0097)  time: 2.1824  data: 1.0675  max mem: 1388\n",
            "Epoch: [0]  [ 90/149]  eta: 0:02:23  lr: 0.000006  loss: 0.5074 (0.5614)  loss_classifier: 0.3580 (0.4233)  loss_box_reg: 0.0817 (0.0707)  loss_objectness: 0.0434 (0.0574)  loss_rpn_box_reg: 0.0095 (0.0100)  time: 2.3584  data: 1.2462  max mem: 1388\n",
            "Epoch: [0]  [100/149]  eta: 0:01:56  lr: 0.000007  loss: 0.4948 (0.5538)  loss_classifier: 0.3345 (0.4136)  loss_box_reg: 0.0432 (0.0698)  loss_objectness: 0.0611 (0.0600)  loss_rpn_box_reg: 0.0115 (0.0103)  time: 2.2057  data: 1.0951  max mem: 1388\n",
            "Epoch: [0]  [110/149]  eta: 0:01:32  lr: 0.000008  loss: 0.4515 (0.5448)  loss_classifier: 0.3209 (0.4045)  loss_box_reg: 0.0734 (0.0708)  loss_objectness: 0.0577 (0.0593)  loss_rpn_box_reg: 0.0073 (0.0102)  time: 2.1283  data: 1.0131  max mem: 1388\n",
            "Epoch: [0]  [120/149]  eta: 0:01:08  lr: 0.000008  loss: 0.4480 (0.5369)  loss_classifier: 0.2956 (0.3951)  loss_box_reg: 0.0972 (0.0727)  loss_objectness: 0.0568 (0.0590)  loss_rpn_box_reg: 0.0073 (0.0101)  time: 2.3034  data: 1.1863  max mem: 1388\n",
            "Epoch: [0]  [130/149]  eta: 0:00:44  lr: 0.000009  loss: 0.4207 (0.5263)  loss_classifier: 0.2832 (0.3852)  loss_box_reg: 0.0840 (0.0724)  loss_objectness: 0.0588 (0.0586)  loss_rpn_box_reg: 0.0085 (0.0101)  time: 2.0533  data: 0.9403  max mem: 1388\n",
            "Epoch: [0]  [140/149]  eta: 0:00:20  lr: 0.000010  loss: 0.3883 (0.5167)  loss_classifier: 0.2609 (0.3759)  loss_box_reg: 0.0561 (0.0738)  loss_objectness: 0.0291 (0.0572)  loss_rpn_box_reg: 0.0054 (0.0098)  time: 1.8483  data: 0.7373  max mem: 1388\n",
            "Epoch: [0]  [148/149]  eta: 0:00:02  lr: 0.000010  loss: 0.3831 (0.5090)  loss_classifier: 0.2483 (0.3683)  loss_box_reg: 0.0783 (0.0735)  loss_objectness: 0.0291 (0.0573)  loss_rpn_box_reg: 0.0049 (0.0098)  time: 1.9694  data: 0.8546  max mem: 1388\n",
            "Epoch: [0] Total time: 0:05:40 (2.2883 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/28]  eta: 0:00:47  model_time: 1.5931 (1.5931)  evaluator_time: 0.0408 (0.0408)  time: 1.6786  data: 0.0418  max mem: 1788\n",
            "Test:  [27/28]  eta: 0:00:01  model_time: 1.5114 (1.5144)  evaluator_time: 0.0338 (0.0345)  time: 1.5930  data: 0.0452  max mem: 1788\n",
            "Test: Total time: 0:00:44 (1.5970 s / it)\n",
            "Averaged stats: model_time: 1.5114 (1.5144)  evaluator_time: 0.0338 (0.0345)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.071\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.066\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.108\n",
            "Epoch: [1]  [  0/149]  eta: 0:02:47  lr: 0.000012  loss: 0.3041 (0.3041)  loss_classifier: 0.2163 (0.2163)  loss_box_reg: 0.0576 (0.0576)  loss_objectness: 0.0240 (0.0240)  loss_rpn_box_reg: 0.0062 (0.0062)  time: 1.1211  data: 0.0363  max mem: 1788\n",
            "Epoch: [1]  [ 10/149]  eta: 0:02:37  lr: 0.000012  loss: 0.3321 (0.3471)  loss_classifier: 0.2163 (0.2161)  loss_box_reg: 0.0661 (0.0687)  loss_objectness: 0.0486 (0.0540)  loss_rpn_box_reg: 0.0062 (0.0083)  time: 1.1311  data: 0.0345  max mem: 1788\n",
            "Epoch: [1]  [ 20/149]  eta: 0:02:26  lr: 0.000012  loss: 0.3297 (0.3385)  loss_classifier: 0.2071 (0.2078)  loss_box_reg: 0.0661 (0.0683)  loss_objectness: 0.0486 (0.0532)  loss_rpn_box_reg: 0.0089 (0.0092)  time: 1.1324  data: 0.0342  max mem: 1788\n",
            "Epoch: [1]  [ 30/149]  eta: 0:02:14  lr: 0.000012  loss: 0.3075 (0.3381)  loss_classifier: 0.1885 (0.1998)  loss_box_reg: 0.0513 (0.0677)  loss_objectness: 0.0484 (0.0608)  loss_rpn_box_reg: 0.0095 (0.0098)  time: 1.1331  data: 0.0344  max mem: 1788\n",
            "Epoch: [1]  [ 40/149]  eta: 0:02:03  lr: 0.000012  loss: 0.3265 (0.3381)  loss_classifier: 0.1822 (0.1948)  loss_box_reg: 0.0692 (0.0712)  loss_objectness: 0.0484 (0.0620)  loss_rpn_box_reg: 0.0082 (0.0101)  time: 1.1333  data: 0.0345  max mem: 1788\n",
            "Epoch: [1]  [ 50/149]  eta: 0:01:52  lr: 0.000012  loss: 0.3203 (0.3302)  loss_classifier: 0.1703 (0.1889)  loss_box_reg: 0.0785 (0.0701)  loss_objectness: 0.0580 (0.0610)  loss_rpn_box_reg: 0.0096 (0.0102)  time: 1.1330  data: 0.0342  max mem: 1788\n",
            "Epoch: [1]  [ 60/149]  eta: 0:01:40  lr: 0.000012  loss: 0.3042 (0.3255)  loss_classifier: 0.1639 (0.1847)  loss_box_reg: 0.0689 (0.0708)  loss_objectness: 0.0519 (0.0598)  loss_rpn_box_reg: 0.0082 (0.0101)  time: 1.1345  data: 0.0343  max mem: 1788\n",
            "Epoch: [1]  [ 70/149]  eta: 0:01:29  lr: 0.000012  loss: 0.2903 (0.3210)  loss_classifier: 0.1602 (0.1803)  loss_box_reg: 0.0689 (0.0712)  loss_objectness: 0.0448 (0.0593)  loss_rpn_box_reg: 0.0082 (0.0102)  time: 1.1338  data: 0.0343  max mem: 1788\n",
            "Epoch: [1]  [ 80/149]  eta: 0:01:18  lr: 0.000012  loss: 0.2908 (0.3169)  loss_classifier: 0.1498 (0.1763)  loss_box_reg: 0.0773 (0.0716)  loss_objectness: 0.0495 (0.0588)  loss_rpn_box_reg: 0.0119 (0.0102)  time: 1.1313  data: 0.0344  max mem: 1788\n",
            "Epoch: [1]  [ 90/149]  eta: 0:01:06  lr: 0.000012  loss: 0.2944 (0.3148)  loss_classifier: 0.1470 (0.1727)  loss_box_reg: 0.0773 (0.0721)  loss_objectness: 0.0597 (0.0597)  loss_rpn_box_reg: 0.0102 (0.0103)  time: 1.1329  data: 0.0348  max mem: 1788\n",
            "Epoch: [1]  [100/149]  eta: 0:00:55  lr: 0.000012  loss: 0.2924 (0.3129)  loss_classifier: 0.1439 (0.1698)  loss_box_reg: 0.0839 (0.0733)  loss_objectness: 0.0577 (0.0596)  loss_rpn_box_reg: 0.0098 (0.0103)  time: 1.1341  data: 0.0352  max mem: 1788\n",
            "Epoch: [1]  [110/149]  eta: 0:00:44  lr: 0.000012  loss: 0.2824 (0.3109)  loss_classifier: 0.1450 (0.1675)  loss_box_reg: 0.0923 (0.0751)  loss_objectness: 0.0445 (0.0584)  loss_rpn_box_reg: 0.0072 (0.0100)  time: 1.1330  data: 0.0347  max mem: 1788\n",
            "Epoch: [1]  [120/149]  eta: 0:00:32  lr: 0.000012  loss: 0.2721 (0.3064)  loss_classifier: 0.1347 (0.1642)  loss_box_reg: 0.0739 (0.0744)  loss_objectness: 0.0445 (0.0579)  loss_rpn_box_reg: 0.0072 (0.0099)  time: 1.1320  data: 0.0342  max mem: 1788\n",
            "Epoch: [1]  [130/149]  eta: 0:00:21  lr: 0.000012  loss: 0.2547 (0.3025)  loss_classifier: 0.1289 (0.1614)  loss_box_reg: 0.0732 (0.0745)  loss_objectness: 0.0491 (0.0570)  loss_rpn_box_reg: 0.0079 (0.0097)  time: 1.1319  data: 0.0348  max mem: 1788\n",
            "Epoch: [1]  [140/149]  eta: 0:00:10  lr: 0.000012  loss: 0.2561 (0.2994)  loss_classifier: 0.1297 (0.1592)  loss_box_reg: 0.0775 (0.0751)  loss_objectness: 0.0297 (0.0556)  loss_rpn_box_reg: 0.0063 (0.0094)  time: 1.1304  data: 0.0346  max mem: 1788\n",
            "Epoch: [1]  [148/149]  eta: 0:00:01  lr: 0.000012  loss: 0.2683 (0.2985)  loss_classifier: 0.1271 (0.1575)  loss_box_reg: 0.0733 (0.0757)  loss_objectness: 0.0287 (0.0557)  loss_rpn_box_reg: 0.0063 (0.0095)  time: 1.1309  data: 0.0345  max mem: 1788\n",
            "Epoch: [1] Total time: 0:02:48 (1.1327 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/28]  eta: 0:00:45  model_time: 1.5453 (1.5453)  evaluator_time: 0.0321 (0.0321)  time: 1.6213  data: 0.0411  max mem: 1788\n",
            "Test:  [27/28]  eta: 0:00:01  model_time: 1.5110 (1.5135)  evaluator_time: 0.0332 (0.0340)  time: 1.5931  data: 0.0454  max mem: 1788\n",
            "Test: Total time: 0:00:44 (1.5952 s / it)\n",
            "Averaged stats: model_time: 1.5110 (1.5135)  evaluator_time: 0.0332 (0.0340)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.037\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.121\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.122\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.108\n",
            "Epoch: [2]  [  0/149]  eta: 0:02:47  lr: 0.000015  loss: 0.2297 (0.2297)  loss_classifier: 0.1251 (0.1251)  loss_box_reg: 0.0805 (0.0805)  loss_objectness: 0.0104 (0.0104)  loss_rpn_box_reg: 0.0136 (0.0136)  time: 1.1222  data: 0.0327  max mem: 1788\n",
            "Epoch: [2]  [ 10/149]  eta: 0:02:37  lr: 0.000015  loss: 0.2436 (0.2623)  loss_classifier: 0.1251 (0.1236)  loss_box_reg: 0.0805 (0.0805)  loss_objectness: 0.0479 (0.0492)  loss_rpn_box_reg: 0.0110 (0.0090)  time: 1.1297  data: 0.0344  max mem: 1788\n",
            "Epoch: [2]  [ 20/149]  eta: 0:02:26  lr: 0.000015  loss: 0.2436 (0.2514)  loss_classifier: 0.1111 (0.1181)  loss_box_reg: 0.0666 (0.0720)  loss_objectness: 0.0479 (0.0516)  loss_rpn_box_reg: 0.0064 (0.0097)  time: 1.1331  data: 0.0345  max mem: 1788\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cdd363e65196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"max_lr\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"gamma\" : 1.0}\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b3a2d62db01c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       train_one_epoch(model, optimizer, train_dl, device, epoch,\n\u001b[0;32m---> 12\u001b[0;31m                       print_freq=10)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 123\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 123\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gF7VNFW9MkW"
      },
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    config = {\n",
        "        \"lr\" : tune.loguniform(5e-4, 1e-3),\n",
        "        \"b1\" : tune.loguniform(0.9, 1),\n",
        "        \"b2\" : tune.loguniform(0.9, 1),\n",
        "        \"base_lr\" : tune.loguniform(1e-4, 1e-6),\n",
        "        \"max_lr\" : tune.loguniform(1e-2, 1e-4),\n",
        "        \"gamma\" : 1.0,\n",
        "        \"batch_size\" : tune.choice([2, 4, 8, 16]),\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric= \"loss\",\n",
        "        mode= \"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        run_or_experiment = train,\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "    \n",
        "    # best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    # device = \"cpu\"\n",
        "    # if torch.cuda.is_available():\n",
        "    #     device = \"cuda:0\"\n",
        "    #     if gpus_per_trial > 1:\n",
        "    #         best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    # best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
        "    model.load_state_dict(model_state)\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}